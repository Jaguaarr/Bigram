# ðŸ”  Bigram
# A Bigram Language Model for Real Name Generation

> A lightweight neural language model built from scratch in PyTorch to learn and generate real names, trained using negative log-likelihood loss.

## ðŸš€ Project Overview

**Bigram** is a simple yet powerful bigram character-level language model that learns from a list of real names and can generate new plausible ones. Minimalistic deep learning approach and is fully implemented using PyTorch.

You can think of it as your own tiny GPT that just wants to name people!

---

## ðŸ§  How it works

1. The model learns character-to-character transitions from a dataset of real names.
2. It uses a bigram (i.e., 2-character context) structure to model probabilities of the next character.
3. It trains a simple neural net to minimize **negative log-likelihood loss (NLL)**.
4. After training, it samples new names character by character.

---

## âœ¨ Example Outputs

Here are some names generated by the model:



Zane
Amira
Lior
Nellie
Kai
Rowan

## ðŸ§  How It Works
Data Preparation: Read a list of names, clean them, and add start/end tokens.

Bigram Counting: For every pair of consecutive characters, count how often they appear.

Probability Matrix: Normalize the counts to probabilities.

Sampling: Generate names by sampling character-by-character using bigram probabilities.

## ðŸ“œ License
MIT License. Free to use, modify, and distribute.
